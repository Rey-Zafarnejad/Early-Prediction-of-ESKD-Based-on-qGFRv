{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting patients' information from Cerner database - Normal and ESKD groups*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * This code belongs to the paper \"Early Prediction of End Stage Kidney Disease Based on Cumulative Estimated Glomerular Filtration Rate Velocity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "import socket    \n",
    "hostname = socket.gethostname()    \n",
    "IPAddr = socket.gethostbyname(hostname)  \n",
    "\n",
    "#conf = SparkConf()\n",
    "conf = SparkConf().setAll([(\"spark.executor.instances\", '5'), ('spark.executor.memory', '8g'), ('spark.executor.cores', '5'), ('spark.driver.memory','4g'),('spark.sql.broadcastTimeout', '3000')])\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('spark-yarn-2')\n",
    "#conf.set(\"spark.driver.host\", '10.42.7.162') #Change it accordingly based on your host ip \n",
    "#address. Open a terminal and use \"cat /etc/hosts\", the last line is the host ip and the host name.\n",
    "conf.set(\"spark.driver.host\", IPAddr)#Change it accordingly based on your host ip address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out the groups from Cerner database\n",
    "\n",
    "## Adult patient full data (with sCr level)\n",
    "data_pool = spark.sql(\"select P.patient_sk, L.lab_drawn_dt_tm as Date, P.race as Race, P.gender as Gender, E.age_in_years as Age, L.numeric_result as sCr_level\\\n",
    "                                      from cerner.orc_hf_d_patient P \\\n",
    "                                      join cerner.orc_hf_f_encounter E on E.patient_id = P.patient_id\\\n",
    "                                      join cerner.orc_hf_f_lab_procedure L on L.encounter_id = E.encounter_id\\\n",
    "                                      where L.detail_lab_procedure_id ='13.0' and L.numeric_result >= 0 and L.numeric_result <= 1000\\\n",
    "                                      and L.numeric_result is not null\\\n",
    "                                      and E.age_in_years >= '18'\\\n",
    "                                      and L.lab_drawn_dt_tm is not null\")\n",
    "                              \n",
    "data_pool.persist()\n",
    "data_pool.cache()\n",
    "#labs_patients_detailed.take(10)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#assuming pat is the initial spark dataframe consisting all information\n",
    "\n",
    "k_female = 0.7\n",
    "k_male = 0.9\n",
    "alpha_male = -0.411\n",
    "alpha_female = -0.329\n",
    "alpha_fixed = -1.209\n",
    "age_factor = 0.993\n",
    "\n",
    "# min(SCr/κ, 1)\n",
    "\n",
    "data_pool=data_pool.withColumn(\"new_sCr\", f.when(f.col('sCr_level') >= f.lit(60), f.col('sCr_level')*f.lit(0.01132))\n",
    ".otherwise(f.col('sCr_level')))\n",
    "\n",
    "data_pool=data_pool.withColumn(\"min(SCr/κ, 1)\", f.when(f.col('Gender') =='Female', f.least(f.col('new_sCr')/k_female, f.lit(1)))\n",
    ".otherwise(f.least(f.col('new_sCr')/k_male, f.lit(1))))\n",
    "\n",
    "# min(SCr/κ, 1)^α\n",
    "data_pool=data_pool.withColumn(\"pow(min(SCr/κ, 1),alpha)\", f.when(f.col('Gender') =='Female', f.pow(f.col('min(SCr/κ, 1)'), f.lit(alpha_female)))\n",
    ".otherwise(f.pow(f.col('min(SCr/κ, 1)'), f.lit(alpha_male))))\n",
    "\n",
    "# max(SCr/κ, 1)\n",
    "data_pool=data_pool.withColumn(\"max(SCr/κ, 1)\", f.when(f.col('Gender') =='Female', f.greatest(f.col('new_sCr')/k_female, f.lit(1)))\n",
    ".otherwise(f.greatest(f.col('new_sCr')/k_male, f.lit(1))))\n",
    "\n",
    "# max(SCr /κ, 1)^-1.209\n",
    "data_pool=data_pool.withColumn(\"pow(max(SCr/κ, 1),alpha)\", f.pow(f.col('max(SCr/κ, 1)'), f.lit(alpha_fixed)))\n",
    "\n",
    "\n",
    "# 0.993^Age\n",
    "data_pool=data_pool.withColumn(\"pow_age\", f.pow(f.lit(age_factor),f.col('Age')))\n",
    "\n",
    "\n",
    "# Intermediate egfrs : 141 x min(SCr/κ, 1)^α x max(SCr /κ, 1)^-1.209 x 0.993^Age\n",
    "data_pool=data_pool.withColumn(\"Intermediate_egfr_1\", 141 * f.col('pow(min(SCr/κ, 1),alpha)'))\n",
    "data_pool=data_pool.withColumn(\"Intermediate_egfr_2\", f.col('Intermediate_egfr_1') * f.col('pow(max(SCr/κ, 1),alpha)'))\n",
    "data_pool=data_pool.withColumn(\"Intermediate_egfr_3\", f.col('Intermediate_egfr_2') * f.col(\"pow_age\"))\n",
    "\n",
    "\n",
    "# Intermediate egfrs : 141 x min(SCr/κ, 1)^α x max(SCr /κ, 1)^-1.209 x 0.993^Age * 1.018[female]\n",
    "data_pool=data_pool.withColumn(\"Intermediate_egfr_4\", f.when(f.col('Gender') =='Female', f.col('Intermediate_egfr_3')* 1.018)\n",
    "                   .otherwise(f.col('Intermediate_egfr_3')))\n",
    "\n",
    "\n",
    "#Final egfrs\n",
    "\n",
    "data_pool=data_pool.withColumn(\"eGFR_EPI\", f.when(f.col('Race') =='Black', f.col('Intermediate_egfr_4')* 1.159)\n",
    "                   .otherwise(f.col('Intermediate_egfr_4')))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "data_pool = data_pool.drop(\"sCr_level\")\n",
    "data_pool = data_pool.drop(\"max(SCr/κ, 1)\")\n",
    "data_pool = data_pool.drop(\"min(SCr/κ, 1)\")\n",
    "data_pool = data_pool.drop(\"pow(min(SCr/κ, 1),alpha)\")\n",
    "data_pool = data_pool.drop(\"pow(max(SCr/κ, 1),alpha)\")\n",
    "data_pool = data_pool.drop(\"pow_age\")\n",
    "data_pool = data_pool.drop(\"Intermediate_egfr_1\")\n",
    "data_pool = data_pool.drop(\"Intermediate_egfr_2\")\n",
    "data_pool = data_pool.drop(\"Intermediate_egfr_3\")\n",
    "data_pool = data_pool.drop(\"Intermediate_egfr_4\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "data_pool = data_pool.withColumn(\"eGFR_bin\", \n",
    "                                 f.when(f.col('eGFR_EPI') < 15, \"[0, 15)-Stage5\")\n",
    "                                 .when((f.col('eGFR_EPI') >= 15) & (f.col('eGFR_EPI') < 30), \"[15, 30)-Stage4\")\n",
    "                                 .when((f.col('eGFR_EPI') >= 30) & (f.col('eGFR_EPI') < 45), \"[30,45)-Stage3B\")\n",
    "                                 .when((f.col('eGFR_EPI') >= 45) & (f.col('eGFR_EPI') < 60), \"[45, 60)-Stage3A\")\n",
    "                                 .when((f.col('eGFR_EPI') >= 60) & (f.col('eGFR_EPI') < 90), \"[60, 90)-Stage2\")\n",
    "                                 .when((f.col('eGFR_EPI') >= 90) & (f.col('eGFR_EPI') < 500), \"[90, ..)-Normal\")\n",
    "                                 .otherwise(None))                  \n",
    "#data_test = data_test.withColumn(\"eGFR_bin_5\", f.when(f.col('eGFR_EPI') < 15, \"[0, 15)-Stage5\").otherwise(None)) \"[45, \n",
    "#data_test = data_test.withColumn(\"eGFR_bin_4\", f.when((f.col('eGFR_EPI') >= 15)&  (f.col('eGFR_EPI') < 30), \"[15, 30)-Stage4\").otherwise(None))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#9 or more datapoints\n",
    "patients_9more = data_pool.groupBy(f.col('patient_sk')).agg(f.count(f.when((f.col('eGFR_bin') == \"[15, 30)-Stage4\")|(f.col('eGFR_bin') == \"[15, 30)-Stage4\")|(f.col('eGFR_bin') == \"[30,45)-Stage3B\")|(f.col('eGFR_bin') == \"[45,60)-Stage3B\")|(f.col('eGFR_bin') == \"[60, 90)-Stage2\"),True)))\n",
    "patients_9more = patients_9more.filter(patients_9more['count(CASE WHEN (((((eGFR_bin = [15, 30)-Stage4) OR (eGFR_bin = [15, 30)-Stage4)) OR (eGFR_bin = [30,45)-Stage3B)) OR (eGFR_bin = [45,60)-Stage3B)) OR (eGFR_bin = [60, 90)-Stage2)) THEN true END)'] >= (9))\n",
    "patients_9more = patients_9more.drop(\"count(CASE WHEN (((((eGFR_bin = [15, 30)-Stage4) OR (eGFR_bin = [15, 30)-Stage4)) OR (eGFR_bin = [30,45)-Stage3B)) OR (eGFR_bin = [45,60)-Stage3B)) OR (eGFR_bin = [60, 90)-Stage2)) THEN true END)\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "data_pool_9more = data_pool.join(patients_9more, on = ['patient_sk'] , how = 'inner')\n",
    "data_pool_9more.cache()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Now, the second criterion (makeing sure this is not acute kidney disease)\n",
    "\n",
    "data_test = data_pool_9more.filter((f.col('eGFR_bin') == \"[15, 30)-Stage4\")|(f.col('eGFR_bin') == \"[15, 30)-Stage4\")|(f.col('eGFR_bin') == \"[30,45)-Stage3B\")|(f.col('eGFR_bin') == \"[45,60)-Stage3B\")|(f.col('eGFR_bin') == \"[60, 90)-Stage2\"))\n",
    "\n",
    "patients_max_date = data_test.groupBy(f.col('patient_sk')).agg(f.max(f.col('Date')))\n",
    "patients_min_date = data_test.groupBy(f.col('patient_sk')).agg(f.min(f.col('Date')))\n",
    "\n",
    "timeFmt = \"YY-mm-dd HH:MM:SS\"\n",
    "patients_not_acute = patients_max_date.join(patients_min_date, on = ['patient_sk']).withColumn(\"Duration\", f.unix_timestamp('max(Date)', format=timeFmt) - f.unix_timestamp('min(Date)',format=timeFmt))\n",
    "patients_not_acute = patients_not_acute.filter(f.col('Duration') > 7776000)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Duration\n",
    "patients_not_acute = patients_not_acute.drop(f.col('max(Date)'))\n",
    "patients_not_acute = patients_not_acute.drop(f.col('min(Date)'))\n",
    "\n",
    "#patients with more than 3 months of data points (not accute)\n",
    "data_pool_9more_chronic = data_pool_9more.join(patients_not_acute.select(patients_not_acute['patient_sk']), on = ['patient_sk'] , how = 'inner')\n",
    "data_pool_9more_chronic.cache()\n",
    "\n",
    "data_pool_9more_chronic_sorted = data_pool_9more_chronic.orderBy('patient_sk', 'Date')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "patients_60to90 = data_pool_9more_chronic_sorted.groupBy(f.col('patient_sk')).agg(f.count(f.when(f.col('eGFR_bin') == \"[60, 90)-Stage2\", True)).alias('count'))\n",
    "patients_60to90 = patients_60to90.filter(patients_60to90['count']>=3)\n",
    "patients_60to90.drop(patients_60to90['count'])\n",
    "\n",
    "data_pool_9more_chronic_sorted_60to90 = data_pool_9more_chronic_sorted.join(patients_60to90.select('patient_sk'), on=['patient_sk'], how='inner')\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ICD9 + ICD 10\n",
    "#Patients With ESRD, Dialysis, Stage 5 CKD\n",
    "\n",
    "Patients_diagnosed_ESRD = spark.sql(\"select distinct P.patient_sk\\\n",
    "                          from cerner.orc_hf_d_diagnosis Dd \\\n",
    "                          join cerner.orc_hf_f_diagnosis Df on Dd.diagnosis_id = Df.diagnosis_id\\\n",
    "                          join cerner.orc_hf_f_encounter E on Df.encounter_id = E.encounter_id\\\n",
    "                          join cerner.orc_hf_d_patient P on E.patient_id = P.patient_id\\\n",
    "                          where Dd.diagnosis_code in ('585.6', 'N18.6')\\\n",
    "                          and E.age_in_years >= '18'\")\n",
    "\n",
    "Patients_diagnosed_ESRD.cache()\n",
    "Patients_diagnosed_ESRD = Patients_diagnosed_ESRD.dropDuplicates()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ICD9 + ICD 10\n",
    "#Patients With ANY diagnosis regrading CKD (different stages or transplent) or Kidney related death (Death caused by Kidney Disease (Nephritis, Nephrotic Syndrome, Nephrosis))\n",
    "\n",
    "Patients_diagnosed_any = spark.sql(\"select distinct P.patient_sk\\\n",
    "                          from cerner.orc_hf_d_diagnosis Dd \\\n",
    "                          join cerner.orc_hf_f_diagnosis Df on Dd.diagnosis_id = Df.diagnosis_id\\\n",
    "                          join cerner.orc_hf_f_encounter E on Df.encounter_id = E.encounter_id\\\n",
    "                          join cerner.orc_hf_d_patient P on E.patient_id = P.patient_id\\\n",
    "                          where Dd.diagnosis_code in ('585.6', '585.5', 'V42.0', 'N18.6', 'Z94.0', 'N18.1', 'N18.2', 'N18.3', 'N18.4', 'N18.5', 'N00', 'N01', 'N02', 'N03', 'N04', 'N05','N06','N07', 'N17', 'N18', 'N19', 'N25', 'N26', 'N27')\\\n",
    "                          and E.age_in_years >= '18'\")\n",
    "\n",
    "Patients_diagnosed_any.cache()\n",
    "Patients_diagnosed_any = Patients_diagnosed_any.dropDuplicates()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Now, splitting up into two groups, again\n",
    "\n",
    "#Normal_group_done = data_pool_9more_chronic_sorted_60to90_age50to80.join(Patients_diagnosed_any.select('patient_sk'), on = ['patient_sk'], how=\"leftanti\")\n",
    "Normal_group_done = data_pool_9more_chronic_sorted_60to90.join(Patients_diagnosed_any.select('patient_sk'), on = ['patient_sk'], how=\"leftanti\")\n",
    "Normal_group_done.cache()\n",
    "\n",
    "#ESRD_group_done = data_pool_9more_chronic_sorted_60to90_age50to80.join(Patients_diagnosed_ESRD.select('patient_sk'), on = ['patient_sk'], how=\"inner\")\n",
    "ESRD_group_done = data_pool_9more_chronic_sorted_60to90.join(Patients_diagnosed_ESRD.select('patient_sk'), on = ['patient_sk'], how=\"inner\")\n",
    "ESRD_group_done.cache()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# The same aim, but now dropping based on eGFR values (first observation below 20)\n",
    "\n",
    "patients_ESRD_eGFR_min_date = ESRD_group_done.groupBy('patient_sk').agg(f.min(f.column('Date')).alias('min_eGFR_date'))\n",
    "ESRD_group_done = ESRD_group_done.join(patients_ESRD_eGFR_min_date, on=['patient_sk'], how='left')\n",
    "\n",
    "ESRD_group_done_first_eGFR = ESRD_group_done.withColumn('first_eGFR', f.when((f.col('min_eGFR_date') == f.col('Date')) , f.col('eGFR_EPI')).otherwise(f.lit(10000)))\n",
    "patients_ESRD_group_done_above_60 =  ESRD_group_done_first_eGFR.filter(f.col('first_eGFR')<1000)\n",
    "patients_ESRD_group_done_above_60 = patients_ESRD_group_done_above_60.filter(f.col('first_eGFR')>=60)\n",
    "ESRD_group_done_above_60 = ESRD_group_done.join(patients_ESRD_group_done_above_60.select('patient_sk'), on=['patient_sk'], how=\"inner\")\n",
    "\n",
    "# The same aim, but now Normal group above 50\n",
    "\n",
    "patients_Normal_group_done_above_60 =  Normal_group_done.groupBy('patient_sk').agg(f.min(f.column('eGFR_EPI')).alias('min_eGFR'))\n",
    "patients_Normal_group_done_above_60 = patients_Normal_group_done_above_60.filter(f.col('min_eGFR')>=60)\n",
    "Normal_group_done_above_60 = Normal_group_done.join(patients_Normal_group_done_above_60.select('patient_sk'), on=['patient_sk'], how=\"inner\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Now, getting rid of the condenced datapoint patients :)\n",
    "\n",
    "Normal_group_done_sparse_above_60 = Normal_group_done_above_60.withColumn('lag', f.lag('Date').over(Window.partitionBy('patient_sk').orderBy('Date')))\n",
    "patients_min_date = Normal_group_done_above_60.groupBy('patient_sk').agg(f.min('Date').alias('mindate'))\n",
    "Normal_group_done_sparse_above_60 = Normal_group_done_sparse_above_60.join(patients_min_date, on = ['patient_sk'], how='left')\n",
    "\n",
    "Normal_group_done_sparse_above_60 = Normal_group_done_sparse_above_60.withColumn('lag_set', f.when(Normal_group_done_sparse_above_60['Date'] > Normal_group_done_sparse_above_60['mindate'], f.col('lag')).otherwise(None))\n",
    "Normal_group_done_sparse_above_60 = Normal_group_done_sparse_above_60.withColumn(\"Duration\", f.abs(f.unix_timestamp('Date', format=timeFmt) - f.unix_timestamp('lag_set',format=timeFmt)))\n",
    "\n",
    "patient_Normal_group_done_sparse = Normal_group_done_sparse_above_60.groupBy('patient_sk').agg(f.min(f.col('Duration')).alias('minn'))\n",
    "patient_Normal_group_done_sparse = patient_Normal_group_done_sparse.filter(f.col('minn')>86400)\n",
    "patient_Normal_group_done_sparse = patient_Normal_group_done_sparse.drop('minn')\n",
    "Normal_group_done_sparse_above_60 = Normal_group_done_above_60.join(patient_Normal_group_done_sparse.select('patient_sk'), on = ['patient_sk'] , how = 'inner')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "## In order to rerun the code, you may want to delet theis part. Here we double check the data extraction with what we expect.\n",
    "# Normal having 9 or more obs AFTER dropping duplicates :)\n",
    "\n",
    "dropped_normal_patients = list(pd.read_csv('dropped_normal_group.csv').patient_sk)\n",
    "dropped_normal_patients = [str(i) for i in dropped_normal_patients]\n",
    "patients_Normal_group_done = Normal_group_done_sparse_above_60.where(f.col(\"patient_sk\").isin(dropped_normal_patients))\n",
    "Normal_group_done_sparse_above_60_droped = Normal_group_done_sparse_above_60.join(patients_Normal_group_done.select('patient_sk'), on = ['patient_sk'] , how = 'leftanti')\n",
    "\n",
    "\n",
    "kept_ESRD_group = list(pd.read_csv('kept_ESRD_group.csv').patient_sk)\n",
    "kept_ESRD_group = [str(i) for i in kept_ESRD_group]\n",
    "patients_ESRD_group_done = ESRD_group_done_above_60.where(f.col(\"patient_sk\").isin(kept_ESRD_group))\n",
    "ESRD_group_done_above_60_droped = ESRD_group_done_above_60.join(patients_ESRD_group_done.select('patient_sk'), on = ['patient_sk'] , how = 'left')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "ESRD_group_done = ESRD_group_done_above_60_droped.dropDuplicates()\n",
    "Normal_group_done = Normal_group_done_sparse_above_60_droped.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESRD_group_done_pandas = ESRD_group_done.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESRD_group_done_pandas.to_csv('Final_ESRD_group_done_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_group_done_pandas = Normal_group_done.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_group_done_pandas.to_csv('Final_Normal_group_done_pandas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For further information please contact rzz5164@psu.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
