{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing 10-fold cross validation and parameter tuning on the CUSUM algorithm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * This code belongs to the paper \"Using CUSUM in real time to signal clinically relevant decreases in estimated glomerular filtration rate\"\n",
    "##### To cite: Zafarnejad, R., Dumbauld, S., Dumbauld, D. et al. Using CUSUM in real time to signal clinically relevant decreases in estimated glomerular filtration rate. BMC Nephrol 23, 287 (2022). https://doi.org/10.1186/s12882-022-02910-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkConf\n",
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.types import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pyspark.sql.functions as f\n",
    "#from pyspark.sql.window import Window\n",
    "import socket    \n",
    "hostname = socket.gethostname()    \n",
    "IPAddr = socket.gethostbyname(hostname)  \n",
    "\n",
    "#conf = SparkConf()\n",
    "#conf = SparkConf().setAll([(\"spark.executor.instances\", '5'), ('spark.executor.memory', '8g'), ('spark.executor.cores', '5'), ('spark.driver.memory','3g'),('spark.sql.broadcastTimeout', '3000')])\n",
    "#conf.setMaster('yarn')\n",
    "#conf.setAppName('spark-yarn-2')\n",
    "#conf.set(\"spark.driver.host\", '10.42.7.162') #Change it accordingly based on your host ip \n",
    "#address. Open a terminal and use \"cat /etc/hosts\", the last line is the host ip and the host name.\n",
    "#conf.set(\"spark.driver.host\", IPAddr)#Change it accordingly based on your host ip address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the datapool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapool_ESRD = pd.read_csv('Final_ESRD_group_done_pandas.csv')\n",
    "datapool_ESRD = datapool_ESRD.drop(columns=datapool_ESRD.columns[0])\n",
    "datapool_ESRD = datapool_ESRD.drop_duplicates()\n",
    "datapool_control = pd.read_csv(\"Final_Normal_group_done_pandas.csv\")\n",
    "datapool_control = datapool_control.drop(columns=datapool_control.columns[0])\n",
    "\n",
    "#some patients have less than 9 datapoinsts!!! AFTER DROPPING DUPLICATES\n",
    "datapool_ESRD_dropped = datapool_ESRD.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index()[datapool_ESRD.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index().eGFR_EPI >=9]\n",
    "datapool_ESRD_dropped = datapool_ESRD_dropped.drop('eGFR_EPI', axis =1)\n",
    "datapool_ESRD = datapool_ESRD.merge(datapool_ESRD_dropped, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "datapool_control = datapool_control.drop(datapool_control.index[np.isinf(datapool_control.eGFR_EPI) == True], axis = 0)\n",
    "datapool_control = datapool_control.drop_duplicates()\n",
    "\n",
    "#some patients have less than 9 datapoinsts!!! AFTER DROPPING DUPLICATES\n",
    "datapool_control_dropped = datapool_control.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index()[datapool_control.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index().eGFR_EPI >=9]\n",
    "datapool_control_dropped = datapool_control_dropped.drop('eGFR_EPI', axis =1)\n",
    "datapool_control = datapool_control.merge(datapool_control_dropped, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "#Pulling out each patient's data \n",
    "#Also. sortinh the data by cSr lavel measurement data and reindexing it\n",
    "\n",
    "patients_list_Normal = list(set(np.unique(list(datapool_control['patient_sk']))))\n",
    "patients_list_ESRD = list(set(np.unique(list(datapool_ESRD['patient_sk']))))\n",
    "\n",
    "\n",
    "# !!!! SHOULD TURN TO TOTAL_SECONDS IN THE MIDST OF ALGORITHM\n",
    "\n",
    "datapool_control['Date'] = pd.to_datetime(datapool_control['Date'])\n",
    "datapool_control['Date'] = pd.to_datetime(datapool_control['Date'])\n",
    "datapool_control_dates = datapool_control.groupby('patient_sk').agg({'Date': 'min'})\n",
    "datapool_control_dates = datapool_control_dates.reset_index()\n",
    "datapool_control = datapool_control.merge(datapool_control_dates, on = 'patient_sk', how='left')\n",
    "datapool_control['Date_seconds'] = (datapool_control['Date_x'] - datapool_control['Date_y'])\n",
    "datapool_control = datapool_control.rename({'Date_x':'Date'}, axis = 1)\n",
    "datapool_control = datapool_control.drop('Date_y', axis = 1)\n",
    "datapool_control['Date_seconds'] = datapool_control['Date_seconds'].dt.total_seconds()\n",
    "\n",
    "datapool_ESRD['Date'] = pd.to_datetime(datapool_ESRD['Date'])\n",
    "datapool_ESRD['Date'] = pd.to_datetime(datapool_ESRD['Date'])\n",
    "datapool_ESRD_dates = datapool_ESRD.groupby('patient_sk').agg({'Date': 'min'})\n",
    "datapool_ESRD_dates = datapool_ESRD_dates.reset_index()\n",
    "datapool_ESRD = datapool_ESRD.merge(datapool_ESRD_dates, on = 'patient_sk', how='left')\n",
    "datapool_ESRD['Date_seconds'] = datapool_ESRD['Date_x'] - datapool_ESRD['Date_y']\n",
    "datapool_ESRD = datapool_ESRD.rename({'Date_x':'Date'}, axis = 1)\n",
    "datapool_ESRD = datapool_ESRD.drop('Date_y', axis = 1)\n",
    "datapool_ESRD['Date_seconds'] = datapool_ESRD['Date_seconds'].dt.total_seconds()\n",
    "\n",
    "#Getting rid of Normal min eGFR < 60\n",
    "\n",
    "datapool_control_patients = datapool_control.groupby('patient_sk').agg({'eGFR_EPI': 'min'})\n",
    "datapool_control_patients = datapool_control_patients[datapool_control_patients['eGFR_EPI']>=60]\n",
    "datapool_control_patients = datapool_control_patients.reset_index()\n",
    "\n",
    "datapool_control = datapool_control_patients.merge(datapool_control, on = 'patient_sk', how = 'inner')\n",
    "datapool_control = datapool_control.rename({'eGFR_EPI_y':'eGFR_EPI'}, axis = 1)\n",
    "datapool_control = datapool_control.drop('eGFR_EPI_x', axis = 1)\n",
    "\n",
    "patients_list_control_above_60 = list(set(np.unique(list(datapool_control['patient_sk']))))\n",
    "\n",
    "patients_list_Normal = patients_list_control_above_60\n",
    "\n",
    "\n",
    "\n",
    "#Getting rid of ESRD min eGFR < 60\n",
    "\n",
    "datapool_ESRD_patients = datapool_ESRD.groupby('patient_sk').agg({'Date': 'min'})\n",
    "datapool_ESRD_patients = datapool_ESRD_patients.reset_index()\n",
    "\n",
    "datapool_ESRD_patients_eGFR = datapool_ESRD.merge(datapool_ESRD_patients, on=['patient_sk', 'Date'], how ='inner')\n",
    "datapool_ESRD_patients_eGFR = datapool_ESRD_patients_eGFR.drop_duplicates('patient_sk')\n",
    "datapool_ESRD_patients_eGFR = datapool_ESRD_patients_eGFR[datapool_ESRD_patients_eGFR['eGFR_EPI']>=60]\n",
    "\n",
    "datapool_ESRD_new = datapool_ESRD.merge(datapool_ESRD_patients_eGFR['patient_sk'], on = 'patient_sk', how = 'inner')\n",
    "\n",
    "datapool_ESRD = datapool_ESRD_new\n",
    "datapool_ESRD = datapool_ESRD.drop_duplicates()\n",
    "patients_list_ESRD = list(set(np.unique(list(datapool_ESRD['patient_sk']))))\n",
    "\n",
    "all_patients = patients_list_Normal + patients_list_ESRD\n",
    "\n",
    "print(len(patients_list_Normal))\n",
    "print(len(patients_list_ESRD))\n",
    "print(len(all_patients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vs. Training sets (0.2 test, 0.8 training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling form the Entire Datapool\n",
    "\n",
    "import random\n",
    "Test_all_patients = random.sample(all_patients, k = 18790)\n",
    "\n",
    "#Re-creating the control and ESRD sets\n",
    "\n",
    "Test_Normal_patients = pd.DataFrame({'patient_sk':list(set(patients_list_Normal).intersection(Test_all_patients))})\n",
    "Testset_control = Test_Normal_patients.merge(datapool_control, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "Test_ESRD_patients = pd.DataFrame({'patient_sk':list(set(patients_list_ESRD).intersection(Test_all_patients))})\n",
    "Testset_ESRD = Test_ESRD_patients.merge(datapool_ESRD, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "Training_Normal_patients = pd.DataFrame({'patient_sk':list(set(patients_list_Normal).difference(Test_all_patients))})\n",
    "Trainingset_control = Training_Normal_patients.merge(datapool_control, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "Training_ESRD_patients = pd.DataFrame({'patient_sk':list(set(patients_list_ESRD).difference(Test_all_patients))})\n",
    "Trainingset_ESRD = Training_ESRD_patients.merge(datapool_ESRD, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "print('Training: ', Training_Normal_patients.shape[0], Training_ESRD_patients.shape[0])\n",
    "print('Test:     ', Test_Normal_patients.shape[0], Test_ESRD_patients.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testset_control.to_csv('Testset_control.csv')\n",
    "Testset_ESRD.to_csv('Testset_ESRD.csv')\n",
    "Trainingset_control.to_csv('Trainingset_control.csv')\n",
    "Trainingset_ESRD.to_csv('Trainingset_ESRD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testset_control = pd.read_csv('Testset_control.csv')\n",
    "Testset_ESRD = pd.read_csv('Testset_ESRD.csv')\n",
    "Trainingset_control = pd.read_csv('Trainingset_control.csv')\n",
    "Trainingset_ESRD = pd.read_csv('Trainingset_ESRD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Normal_patients = pd.DataFrame({'patient_sk':list(Trainingset_control.patient_sk.unique())})\n",
    "Training_ESRD_patients = pd.DataFrame({'patient_sk':list(Trainingset_ESRD.patient_sk.unique())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And The Oscar goes to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 folds, 1 time  (1*1*1), ~33 seconds\n",
    "#10 folds, 8 times (2*2*2), ~220 seconds , second try = 177 seconds! , third try = 167 seconds!!! , forth: 161 !!!(-> linearly speaking it should be 264 seconds)\n",
    "#10 folds, 9,261 times (7*21*15), ~220 seconds -> linearly speaking it should be 2,205 seconds\n",
    "\n",
    "import random\n",
    "from numba import jit\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "V0_list = [0.0]\n",
    "w_list = [0.0, 0.25, 0.5, 0.6, 0.75, 1.0]\n",
    "T_list = [-3.0, -4.0, -5.0, -6.0, -7.0, -8.0]\n",
    "#a_list = np.linspace(0.0, 1.0,  num = 21)\n",
    "a = 0.2\n",
    "\n",
    "k_folds = 10\n",
    "\n",
    "Mu_list = []\n",
    "Sigma_list = []\n",
    "            \n",
    "sampled_measurements = pd.DataFrame({}, index = [])\n",
    "        \n",
    "for i in range(k_folds):\n",
    "    \n",
    "    sampled_Normal_patients = random.sample(list(Training_Normal_patients.patient_sk), k = Training_ESRD_patients.shape[0])\n",
    "#    sampled_Normal_patients = pd.DataFrame({'patient_sk':list(set(patients_list_Normal).intersection(sampled_Normal_patients))})\n",
    "    sampled_Normal_patients = pd.DataFrame({'patient_sk':sampled_Normal_patients})\n",
    "    Trainingset_control_sampled = sampled_Normal_patients.merge(Trainingset_control, on = 'patient_sk', how = 'inner')\n",
    "    \n",
    "    \n",
    "    #Sampled Mu and sigma\n",
    "\n",
    "    var_list = []\n",
    "    n_list = []\n",
    "\n",
    "    mu = np.mean(Trainingset_control_sampled['eGFR_EPI'])\n",
    "\n",
    "    var_list = Trainingset_control_sampled.groupby('patient_sk').agg({'eGFR_EPI':'std'})\n",
    "    var_list = list(var_list.eGFR_EPI)\n",
    "\n",
    "    n_list =  Trainingset_control_sampled.groupby('patient_sk').agg({'patient_sk':'count'})\n",
    "    n_list = list(n_list.patient_sk)\n",
    "    #calculating the mean and variance of the Normal sample\n",
    "\n",
    "    n_1 = list((n_list - np.ones(len(n_list))).astype('int'))\n",
    "    numerator = np.multiply(n_1, np.power(var_list, 2))\n",
    "    denominator = sum(n_list) - len(n_list)\n",
    "    sigma = np.power(sum(numerator)/denominator,0.5)\n",
    "\n",
    "    \n",
    "    Mu_list.append(mu)\n",
    "    Sigma_list.append(sigma)\n",
    "    \n",
    "    \n",
    "    Accuracy_list = []\n",
    "    Sensetivity_list = []\n",
    "    Specificity_list = []\n",
    " \n",
    "\n",
    "    for V0 in V0_list:\n",
    "        for w in w_list:\n",
    "            for T in T_list:\n",
    "                \n",
    "                datapool_control = Trainingset_control_sampled\n",
    "                datapool_ESRD = Trainingset_ESRD\n",
    "    ## Zi:\n",
    "                datapool_control['Zi'] = (datapool_control.eGFR_EPI - mu)/sigma\n",
    "                datapool_ESRD['Zi'] = (datapool_ESRD.eGFR_EPI - mu)/sigma\n",
    "\n",
    "    ## AAANNNDDD let us start palying with Zi and Vi :) AND THE SLOPES AS WELL :)\n",
    "\n",
    "                @jit(nopython=True)\n",
    "                def Vi_creator(Zi, patient_sk):\n",
    "                    Vi = np.zeros(Zi.shape)\n",
    "                    Vi[0] = V0\n",
    "\n",
    "                    for i in range(1, Vi.shape[0]):\n",
    "                        if patient_sk[i] == patient_sk[i-1]:\n",
    "                            Vi[i] = (min(0.0, Zi[i] + w + Vi[i-1]))\n",
    "                        else:\n",
    "                            Vi[i] = V0\n",
    "\n",
    "                    return Vi\n",
    "\n",
    "                datapool_control['Vi'] = Vi_creator(datapool_control['Zi'].values, datapool_control['patient_sk'].values)\n",
    "                datapool_ESRD['Vi'] = Vi_creator(datapool_ESRD['Zi'].values, datapool_ESRD['patient_sk'].values)\n",
    "\n",
    "    ### OMG! The Slopes!\n",
    "\n",
    "                Inst_slope_initial = 0.0\n",
    "                Smooth_slope_initial = 0.0\n",
    "\n",
    "                @jit(nopython=True)\n",
    "                def Slope_creator(Vi, patient_sk, Date_seconds, eGFR_EPI):\n",
    "                    Inst_slope = np.zeros(Vi.shape)\n",
    "                    Smooth_slope = np.zeros(Vi.shape)\n",
    "\n",
    "                    Inst_slope[0] = Inst_slope_initial\n",
    "                    Smooth_slope[0] = Smooth_slope_initial\n",
    "\n",
    "                    for i in range(1, Vi.shape[0]):\n",
    "                        if patient_sk[i] == patient_sk[i-1]:\n",
    "                            if Vi[i-1] == 0.0 :\n",
    "                                if Date_seconds[i] - Date_seconds[i-1] != 0:\n",
    "                                    Inst_slope[i] = min(0.0, (eGFR_EPI[i] - mu)/((Date_seconds[i] - Date_seconds[i-1])/86400))\n",
    "                                else:\n",
    "                                    Inst_slope[i] = Inst_slope[i-1]\n",
    "                            else:\n",
    "                                if Date_seconds[i] - Date_seconds[i-1] != 0:\n",
    "                                    Inst_slope[i] = min(0.0, (eGFR_EPI[i] - eGFR_EPI[i-1])/((Date_seconds[i] - Date_seconds[i-1])/86400))\n",
    "                                else:\n",
    "                                    Inst_slope[i] = Inst_slope[i-1]\n",
    "                            if Date_seconds[i] - Date_seconds[i-1] != 0:\n",
    "                                Smooth_slope[i] = (1-a) * Smooth_slope[i-1] + a * (min(0.0, (eGFR_EPI[i] - eGFR_EPI[i-1])/((Date_seconds[i] - Date_seconds[i-1])/86400)))\n",
    "                            else:\n",
    "                                Smooth_slope[i] = Smooth_slope[i-1]\n",
    "                        else:\n",
    "                            Inst_slope[i] = Inst_slope_initial\n",
    "                            Smooth_slope[i] = Smooth_slope_initial\n",
    "\n",
    "                    return [Inst_slope, Smooth_slope]\n",
    "\n",
    "                [datapool_control['Inst_slope'],datapool_control['Smooth_slope']] = Slope_creator(datapool_control['Vi'].values, datapool_control['patient_sk'].values, datapool_control['Date_seconds'].values, datapool_control['eGFR_EPI'].values)\n",
    "                [datapool_ESRD['Inst_slope'], datapool_ESRD['Smooth_slope']] = Slope_creator(datapool_ESRD['Vi'].values, datapool_ESRD['patient_sk'].values, datapool_ESRD['Date_seconds'].values, datapool_ESRD['eGFR_EPI'].values)\n",
    "\n",
    "\n",
    "    # Making up the result trigger date and eGFR tables\n",
    "\n",
    "                patients_control_trigger = datapool_control[datapool_control['Vi'] <= T].groupby('patient_sk').agg({'Date': 'min'})\n",
    "                patients_control_trigger = patients_control_trigger.reset_index()\n",
    "                patients_control_trigger = patients_control_trigger.merge(datapool_control[['patient_sk', 'eGFR_EPI', 'Date']], on=['patient_sk', 'Date'], how='inner')\n",
    "                patients_control_trigger = patients_control_trigger.drop_duplicates('patient_sk')\n",
    "                patients_control_trigger['New_label'] = list(np.ones(len(patients_control_trigger)))\n",
    "\n",
    "                patients_ESRD_trigger = datapool_ESRD[datapool_ESRD['Vi'] <= T].groupby('patient_sk').agg({'Date': 'min'})\n",
    "                patients_ESRD_trigger = patients_ESRD_trigger.reset_index()\n",
    "                patients_ESRD_trigger = patients_ESRD_trigger.merge(datapool_ESRD[['patient_sk', 'eGFR_EPI', 'Date']], on=['patient_sk', 'Date'], how='inner')\n",
    "                patients_ESRD_trigger = patients_ESRD_trigger.drop_duplicates('patient_sk')\n",
    "                patients_ESRD_trigger['New_label'] = list(np.ones(len(patients_ESRD_trigger)))\n",
    "\n",
    "\n",
    "    #Labeling and finishing :)\n",
    "\n",
    "                patients_Normal_labeled = pd.DataFrame({'patient_sk' : list(sampled_Normal_patients.patient_sk) , 'Label' : list(np.zeros(len(sampled_Normal_patients)))})\n",
    "                patients_Normal_labeled = patients_Normal_labeled.merge(patients_control_trigger, on='patient_sk', how='left')\n",
    "\n",
    "                patients_ESRD_labeled = pd.DataFrame({'patient_sk' : list(Training_ESRD_patients.patient_sk) , 'Label' : list(np.ones(len(Training_ESRD_patients)))})\n",
    "                patients_ESRD_labeled = patients_ESRD_labeled.merge(patients_ESRD_trigger, on='patient_sk', how='left')\n",
    "\n",
    "\n",
    "    #Accuracy, Sensetivity, Specificity\n",
    "                # ESRD NaN = 0.0\n",
    "                # Normal NaN = 0.0\n",
    "\n",
    "                #RIGHT detection in ESRD:\n",
    "                numbet_of_ones_ESRD = patients_ESRD_labeled[patients_ESRD_labeled['New_label'] == 1].shape[0]\n",
    "\n",
    "                #WRONG detection in Normal\n",
    "                numbet_of_ones_Normal = patients_Normal_labeled[patients_Normal_labeled['New_label'] == 1].shape[0]\n",
    "\n",
    "                total_ESRD = patients_ESRD_labeled.shape[0]\n",
    "                total_Normal = patients_Normal_labeled.shape[0]\n",
    "\n",
    "\n",
    "                # Accuracy\n",
    "                Accuracy = (numbet_of_ones_ESRD + (total_Normal - numbet_of_ones_Normal))/(total_ESRD + total_Normal)\n",
    "\n",
    "                #Sensetivity\n",
    "                tp = numbet_of_ones_ESRD\n",
    "                fn = total_ESRD - numbet_of_ones_ESRD\n",
    "                Sensetivity = tp/(tp+fn)\n",
    "\n",
    "                #Specificity\n",
    "                tn = total_Normal - numbet_of_ones_Normal\n",
    "                fp = numbet_of_ones_Normal\n",
    "                Specificity = tn/(tn+fp)\n",
    "\n",
    "                Accuracy_list.append(Accuracy)\n",
    "                Sensetivity_list.append(Sensetivity)\n",
    "                Specificity_list.append(Specificity)\n",
    "                \n",
    "                \n",
    "    sampled_measurements['Accuracy_fold_{}'.format(i)] = Accuracy_list\n",
    "    sampled_measurements['Sensetivity_fold_{}'.format(i)] = Sensetivity_list\n",
    "    sampled_measurements['Specificity_fold_{}'.format(i)] = Specificity_list\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "combination_list = []\n",
    "for V0 in V0_list:\n",
    "    for w in w_list:\n",
    "        for T in T_list:\n",
    "            a = 'V0={}, w={}, T={}'.format(V0, w, T)\n",
    "            combinations.append(a)\n",
    "            combination_list.append([V0, w, T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_measurements['Combinations'] = combinations\n",
    "sampled_measurements = sampled_measurements.set_index('Combinations')\n",
    "sampled_measurements_pruned = sampled_measurements.loc[['V0=0.0, w=0.0, T=-3.0', 'V0=0.0, w=0.0, T=-4.0','V0=0.0, w=0.25, T=-3.0','V0=0.0, w=0.25, T=-4.0','V0=0.0, w=0.5, T=-4.0','V0=0.0, w=0.5, T=-5.0', 'V0=0.0, w=0.75, T=-4.0','V0=0.0, w=0.75, T=-5.0', 'V0=0.0, w=0.75, T=-6.0','V0=0.0, w=1.0, T=-6.0','V0=0.0, w=1.0, T=-7.0']]\n",
    "sampled_measurements_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_measurements_pruned.to_csv('sampled_measurements_v0_neg_10_newwww.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Accuracy = np.array([list(np.zeros(sampled_measurements_pruned.shape[0]))])\n",
    "Mean_Sensetivity = np.array([list(np.zeros(sampled_measurements_pruned.shape[0]))])\n",
    "Mean_Specificity = np.array([list(np.zeros(sampled_measurements_pruned.shape[0]))])\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    Mean_Accuracy = Mean_Accuracy + np.array(list(sampled_measurements_pruned['Accuracy_fold_{}'.format(i)]))/((sampled_measurements_pruned.shape[1])/3)\n",
    "    Mean_Sensetivity = Mean_Sensetivity + np.array(list(sampled_measurements_pruned['Sensetivity_fold_{}'.format(i)]))/((sampled_measurements_pruned.shape[1] )/3)\n",
    "    Mean_Specificity = Mean_Specificity + np.array(list(sampled_measurements_pruned['Specificity_fold_{}'.format(i)]))/((sampled_measurements_pruned.shape[1] )/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table = pd.DataFrame({'Combinations' : ['w=0.0, T=-3.0', 'w=0.0, T=-4.0','w=0.25, T=-3.0','w=0.25, T=-4.0','w=0.5, T=-4.0','w=0.5, T=-5.0', 'w=0.75, T=-4.0', 'w=0.75, T=-5.0','w=0.75, T=-6.0','w=1.0, T=-6.0','w=1.0, T=-7.0'] , 'Mean_Accuracy' : list(Mean_Accuracy[0]) , 'Mean_Sensetivity' : list(Mean_Sensetivity[0]) , 'Mean_Specificity' : list(Mean_Specificity[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table.to_csv('sampled_measurements_v0_neg_10_newwww.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table[Combination_wise_table.Mean_Specificity == np.max(Combination_wise_table.Mean_Specificity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC curve (Receiver Operating Characteristic)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "    \n",
    "Sensetivity_list = list(Combination_wise_table['Mean_Sensetivity'])\n",
    "Specificity_list = list(Combination_wise_table['Mean_Specificity'])\n",
    "Accuracy_list = list(Combination_wise_table['Mean_Accuracy'])\n",
    "\n",
    "max_accuracy = max(Accuracy_list)\n",
    "max_index = Accuracy_list.index(max_accuracy)\n",
    "comcom = combinations[max_index]\n",
    "\n",
    "FPR = list(np.ones(len(Specificity_list)) - Specificity_list)\n",
    "\n",
    "ax.plot(list(FPR), Sensetivity_list, color=\"darkslategray\", lw=2, ls='-', marker='o', markersize=8, markerfacecolor=\"darkslategray\")\n",
    "\n",
    "ax.set_xlabel('FPR = 1 - Specificity')\n",
    "ax.set_ylabel('TPR = Sensitivity')\n",
    "\n",
    "#ax.set_title('Max Accuracy = {}, with {}'.format(max_accuracy, comcom))\n",
    "\n",
    "i = 0\n",
    "\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlim([0,1])\n",
    "\n",
    "i = 20\n",
    "for label, x, y in zip(Combination_wise_table['Combinations'], list(FPR), Sensetivity_list):\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(75, -i),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.2', fc='lightgray', alpha=1,color='darkgray'),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0',color='darkgray',))    \n",
    "    i = i + 16\n",
    "    \n",
    "\n",
    "plt.savefig('plot2021_ROC.jpg', orientation=\"landscape\",\n",
    "           dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data_sampling = pd.DataFrame({'[V0, w, T]':combination_list,'Sample No. 0' : Accuracy_FULL[0], 'Sample No. 1' : Accuracy_FULL[1], 'Sample No. 2' : Accuracy_FULL[2], 'Sample No. 3' : Accuracy_FULL[3], 'Sample No. 4' : Accuracy_FULL[4], 'Sample No. 5' : Accuracy_FULL[5], 'Sample No. 6' : Accuracy_FULL[6], 'Sample No. 7' : Accuracy_FULL[7], 'Sample No. 8' : Accuracy_FULL[8], 'Sample No. 9' : Accuracy_FULL[9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensetivity_data_sampling = pd.DataFrame({'[V0, w, T]':combination_list, 'Sample No. 0' : Sensetivity_FULL[0], 'Sample No. 1' : Sensetivity_FULL[1], 'Sample No. 2' : Sensetivity_FULL[2], 'Sample No. 3' : Sensetivity_FULL[3], 'Sample No. 4' : Sensetivity_FULL[4], 'Sample No. 5' : Sensetivity_FULL[5], 'Sample No. 6' : Sensetivity_FULL[6], 'Sample No. 7' : Sensetivity_FULL[7], 'Sample No. 8' : Sensetivity_FULL[8], 'Sample No. 9' : Sensetivity_FULL[9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_data_sampling = pd.DataFrame({'[V0, w, T]':combination_list, 'Sample No. 0' : Specificity_FULL[0], 'Sample No. 1' : Specificity_FULL[1], 'Sample No. 2' : Specificity_FULL[2], 'Sample No. 3' : Specificity_FULL[3], 'Sample No. 4' : Specificity_FULL[4], 'Sample No. 5' : Specificity_FULL[5], 'Sample No. 6' : Specificity_FULL[6], 'Sample No. 7' : Specificity_FULL[7], 'Sample No. 8' : Specificity_FULL[8], 'Sample No. 9' : Sensetivity_FULL[9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_initial_data = pd.DataFrame({'Mu\"s' : mu_sampled, 'Sigma\"s' :sigma_sampled, 'Samples':patient_sampled_total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_sampled = []\n",
    "patient_sampled_total = []\n",
    "for dataaa_control in dataaa_control_sampled:\n",
    "    for patient in dataaa_control:    \n",
    "        patient_sampled.append(patient['patient_sk'][0])\n",
    "        \n",
    "    patient_sampled_total.append(patient_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_data_sampling.to_csv('accuracy_data_sampling_pandas.csv')\n",
    "#sensetivity_data_sampling.to_csv('sensetivity_data_sampling_pandas.csv')\n",
    "#specificity_data_sampling.to_csv('specificity_data_sampling_pandas.csv')\n",
    "#sampling_initial_data.to_csv('mu_and_sigma_sampled_pandas.csv')\n",
    "sampling_initial_data.to_csv('sampling_initial_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC curve (Receiver Operating Characteristic)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=1, figsize = (8,60))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for ax in axes:\n",
    "    \n",
    "    Specificity_list = Specificity_FULL[i]\n",
    "    Specificity_list = Specificity_FULL[i]\n",
    "    Accuracy_list = Accuracy_FULL[i]\n",
    "    \n",
    "    max_accuracy = max(Accuracy_list)\n",
    "    max_index = Accuracy_list.index(max_accuracy)\n",
    "    comcom = combinations[max_index]\n",
    "    \n",
    "    FPR = np.ones(len(Specificity_list)) - Specificity_list\n",
    "\n",
    "    ax.plot(FPR, Sensetivity_list, color=\"orchid\", lw=2, ls='-', marker='o', markersize=8, markerfacecolor=\"purple\")\n",
    "\n",
    "    ax.set_xlabel('FPR = 1 - Specificity')\n",
    "    ax.set_ylabel('TPR = Sensetivity')\n",
    "    \n",
    "    ax.set_title('Max Accuracy = {}, with {}'.format(max_accuracy, comcom))\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    #ax.set_ylim([0,1])\n",
    "    #ax.set_xlim([0,1])\n",
    "    \n",
    "    for label, x, y in zip(combinations, FPR, Sensetivity_list):\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-15, 15),\n",
    "            textcoords='offset points', ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=1),\n",
    "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot them! once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb0 = pd.read_csv('Combination_wise_table_v0_0.csv')\n",
    "comb05 = pd.read_csv('Combination_wise_table_v0_neg_0_5.csv')\n",
    "comb1 = pd.read_csv('Combination_wise_table_v0_neg_1.csv')\n",
    "comb15 = pd.read_csv('Combination_wise_table_v0_neg_1_5.csv')\n",
    "comb2 = pd.read_csv('Combination_wise_table_v0_neg_2.csv')\n",
    "comb25 = pd.read_csv('Combination_wise_table_v0_neg_2_5.csv')\n",
    "comb3 = pd.read_csv('Combination_wise_table_v0_neg_3.csv')\n",
    "comb35 = pd.read_csv('Combination_wise_table_v0_neg_3_5.csv')\n",
    "comb4 = pd.read_csv('Combination_wise_table_v0_neg_4.csv')\n",
    "comb45 = pd.read_csv('Combination_wise_table_v0_neg_4_5.csv')\n",
    "comb5 = pd.read_csv('Combination_wise_table_v0_neg_5.csv')\n",
    "comb55 = pd.read_csv('Combination_wise_table_v0_neg_5_5.csv')\n",
    "comb6 = pd.read_csv('Combination_wise_table_v0_neg_6.csv')\n",
    "comb75 = pd.read_csv('Combination_wise_table_v0_neg_7_5.csv')\n",
    "comb8 = pd.read_csv('Combination_wise_table_v0_neg_8.csv')\n",
    "comb85 = pd.read_csv('Combination_wise_table_v0_neg_8_5.csv')\n",
    "comb9 = pd.read_csv('Combination_wise_table_v0_neg_9.csv')\n",
    "comb95 = pd.read_csv('Combination_wise_table_v0_neg_9_5.csv')\n",
    "comb10 = pd.read_csv('Combination_wise_table_v0_neg_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table = pd.concat([comb0, comb05, comb1, comb15, comb2, comb25, comb3, comb35, comb4, comb45, comb5, comb55, comb6, comb75, comb8, comb85, comb9, comb95, comb10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table = Combination_wise_table.drop(columns=Combination_wise_table.columns[0])\n",
    "Combination_wise_table = Combination_wise_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC curve (Receiver Operating Characteristic)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (10,10))\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "    \n",
    "Sensetivity_list = list(Combination_wise_table['Mean_Sensetivity'])\n",
    "Specificity_list = list(Combination_wise_table['Mean_Specificity'])\n",
    "Accuracy_list = list(Combination_wise_table['Mean_Accuracy'])\n",
    "\n",
    "max_accuracy = max(Accuracy_list)\n",
    "max_index = Accuracy_list.index(max_accuracy)\n",
    "comcom = combinations[max_index]\n",
    "\n",
    "FPR = list(np.ones(len(Specificity_list)) - Specificity_list)\n",
    "\n",
    "ax.plot(list(FPR), Sensetivity_list, color=\"orchid\", lw=2, ls='-', marker='o', markersize=8, markerfacecolor=\"purple\")\n",
    "\n",
    "ax.set_xlabel('FPR = 1 - Specificity')\n",
    "ax.set_ylabel('TPR = Sensetivity')\n",
    "\n",
    "ax.set_title('Max Accuracy = {}, with {}'.format(max_accuracy, comcom))\n",
    "\n",
    "i = 0\n",
    "\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlim([0,1])\n",
    "\n",
    "#for label, x, y in zip(Combination_wise_table['Combinations'], list(FPR), Sensetivity_list):\n",
    "#    ax.annotate(\n",
    "#        label,\n",
    "#        xy=(x, y), xytext=(-15, 15),\n",
    "#        textcoords='offset points', ha='right', va='bottom',\n",
    "#        bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=1),\n",
    "#        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table[Combination_wise_table.Mean_Accuracy == np.max(Combination_wise_table.Mean_Accuracy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Sensetivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table[Combination_wise_table.Mean_Sensetivity == np.max(Combination_wise_table.Mean_Sensetivity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Sensetivity (less than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comcom = Combination_wise_table[Combination_wise_table.Mean_Sensetivity < 1]\n",
    "comcom[comcom.Mean_Sensetivity == np.max(comcom.Mean_Sensetivity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table[Combination_wise_table.Mean_Specificity == np.max(Combination_wise_table.Mean_Specificity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wanna see them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "#Plot Data\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sns.distplot(Combination_wise_table.Mean_Accuracy, bins=25, color=\"y\", ax=ax )\n",
    "ax.set(xlabel=\"The distribution of accuracy\", ylabel = \"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "#Plot Data\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sns.distplot(Combination_wise_table.Mean_Sensetivity, bins=25, color=\"orange\", ax=ax )\n",
    "ax.set(xlabel=\"The distribution of sensetivity\", ylabel = \"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "#Plot Data\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sns.distplot(Combination_wise_table.Mean_Specificity, bins=25, color=\"red\", ax=ax )\n",
    "ax.set(xlabel=\"The distribution of specificity\", ylabel = \"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best point (sens and spec more than 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_wise_table[(Combination_wise_table.Mean_Sensetivity >= 0.89) & (Combination_wise_table.Mean_Specificity >= 0.89) & (Combination_wise_table.Mean_Accuracy >= 0.88)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For further information please contact rzz5164@psu.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
