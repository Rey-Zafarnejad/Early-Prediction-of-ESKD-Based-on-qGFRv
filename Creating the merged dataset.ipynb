{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the \"merged dataset\" which contains patient IDs as well as qGFRv and trigger points data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * This code belongs to the paper \"Using CUSUM in real time to signal clinically relevant decreases in estimated glomerular filtration rate\"\n",
    "##### To cite: Zafarnejad, R., Dumbauld, S., Dumbauld, D. et al. Using CUSUM in real time to signal clinically relevant decreases in estimated glomerular filtration rate. BMC Nephrol 23, 287 (2022). https://doi.org/10.1186/s12882-022-02910-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "import socket    \n",
    "hostname = socket.gethostname()    \n",
    "IPAddr = socket.gethostbyname(hostname)  \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#conf = SparkConf()\n",
    "conf = SparkConf().setAll([(\"spark.executor.instances\", '5'), ('spark.executor.memory', '8g'), ('spark.executor.cores', '5'), ('spark.driver.memory','3g'),('spark.sql.broadcastTimeout', '3000')])\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('spark-yarn-2')\n",
    "#conf.set(\"spark.driver.host\", '10.42.7.162') #Change it accordingly based on your host ip \n",
    "#address. Open a terminal and use \"cat /etc/hosts\", the last line is the host ip and the host name.\n",
    "conf.set(\"spark.driver.host\", IPAddr)#Change it accordingly based on your host ip address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapool_ESRD = pd.read_csv('Final_ESRD_group_done_pandas.csv')\n",
    "datapool_ESRD = datapool_ESRD.drop(columns=datapool_ESRD.columns[0])\n",
    "datapool_ESRD = datapool_ESRD.drop_duplicates()\n",
    "datapool_control = pd.read_csv(\"Final_Normal_group_done_pandas.csv\")\n",
    "datapool_control = datapool_control.drop(columns=datapool_control.columns[0])\n",
    "\n",
    "#some patients have less than 9 datapoinsts!!! AFTER DROPPING DUPLICATES\n",
    "datapool_ESRD_dropped = datapool_ESRD.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index()[datapool_ESRD.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index().eGFR_EPI >=9]\n",
    "datapool_ESRD_dropped = datapool_ESRD_dropped.drop('eGFR_EPI', axis =1)\n",
    "datapool_ESRD = datapool_ESRD.merge(datapool_ESRD_dropped, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "datapool_control = datapool_control.drop(datapool_control.index[np.isinf(datapool_control.eGFR_EPI) == True], axis = 0)\n",
    "datapool_control = datapool_control.drop_duplicates()\n",
    "\n",
    "#some patients have less than 9 datapoinsts!!! AFTER DROPPING DUPLICATES\n",
    "datapool_control_dropped = datapool_control.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index()[datapool_control.groupby('patient_sk').agg({'eGFR_EPI' : 'count'}).reset_index().eGFR_EPI >=9]\n",
    "datapool_control_dropped = datapool_control_dropped.drop('eGFR_EPI', axis =1)\n",
    "datapool_control = datapool_control.merge(datapool_control_dropped, on = 'patient_sk', how = 'inner')\n",
    "\n",
    "#Pulling out each patient's data \n",
    "#Also. sortinh the data by cSr lavel measurement data and reindexing it\n",
    "\n",
    "patients_list_Normal = list(set(np.unique(list(datapool_control['patient_sk']))))\n",
    "patients_list_ESRD = list(set(np.unique(list(datapool_ESRD['patient_sk']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! SHOULD TURN TO TOTAL_SECONDS IN THE MIDST OF ALGORITHM\n",
    "\n",
    "datapool_control['Date'] = pd.to_datetime(datapool_control['Date'])\n",
    "datapool_control['Date'] = pd.to_datetime(datapool_control['Date'])\n",
    "datapool_control_dates = datapool_control.groupby('patient_sk').agg({'Date': 'min'})\n",
    "datapool_control_dates = datapool_control_dates.reset_index()\n",
    "datapool_control = datapool_control.merge(datapool_control_dates, on = 'patient_sk', how='left')\n",
    "datapool_control['Date_seconds'] = (datapool_control['Date_x'] - datapool_control['Date_y'])\n",
    "datapool_control = datapool_control.rename({'Date_x':'Date'}, axis = 1)\n",
    "datapool_control = datapool_control.drop('Date_y', axis = 1)\n",
    "datapool_control['Date_seconds'] = datapool_control['Date_seconds'].dt.total_seconds()\n",
    "\n",
    "datapool_ESRD['Date'] = pd.to_datetime(datapool_ESRD['Date'])\n",
    "datapool_ESRD['Date'] = pd.to_datetime(datapool_ESRD['Date'])\n",
    "datapool_ESRD_dates = datapool_ESRD.groupby('patient_sk').agg({'Date': 'min'})\n",
    "datapool_ESRD_dates = datapool_ESRD_dates.reset_index()\n",
    "datapool_ESRD = datapool_ESRD.merge(datapool_ESRD_dates, on = 'patient_sk', how='left')\n",
    "datapool_ESRD['Date_seconds'] = datapool_ESRD['Date_x'] - datapool_ESRD['Date_y']\n",
    "datapool_ESRD = datapool_ESRD.rename({'Date_x':'Date'}, axis = 1)\n",
    "datapool_ESRD = datapool_ESRD.drop('Date_y', axis = 1)\n",
    "datapool_ESRD['Date_seconds'] = datapool_ESRD['Date_seconds'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of Normal min eGFR < 60\n",
    "\n",
    "datapool_control_patients = datapool_control.groupby('patient_sk').agg({'eGFR_EPI': 'min'})\n",
    "datapool_control_patients = datapool_control_patients[datapool_control_patients['eGFR_EPI']>=60]\n",
    "datapool_control_patients = datapool_control_patients.reset_index()\n",
    "\n",
    "datapool_control = datapool_control_patients.merge(datapool_control, on = 'patient_sk', how = 'inner')\n",
    "datapool_control = datapool_control.rename({'eGFR_EPI_y':'eGFR_EPI'}, axis = 1)\n",
    "datapool_control = datapool_control.drop('eGFR_EPI_x', axis = 1)\n",
    "\n",
    "patients_list_control_above_50 = list(set(np.unique(list(datapool_control['patient_sk']))))\n",
    "\n",
    "patients_list_Normal = patients_list_control_above_50\n",
    "\n",
    "\n",
    "\n",
    "#Getting rid of ESRD min eGFR < 60\n",
    "\n",
    "datapool_ESRD_patients = datapool_ESRD.groupby('patient_sk').agg({'Date': 'min'})\n",
    "datapool_ESRD_patients = datapool_ESRD_patients.reset_index()\n",
    "\n",
    "datapool_ESRD_patients_eGFR = datapool_ESRD.merge(datapool_ESRD_patients, on=['patient_sk', 'Date'], how ='inner')\n",
    "datapool_ESRD_patients_eGFR = datapool_ESRD_patients_eGFR.drop_duplicates('patient_sk')\n",
    "datapool_ESRD_patients_eGFR = datapool_ESRD_patients_eGFR[datapool_ESRD_patients_eGFR['eGFR_EPI']>=60]\n",
    "\n",
    "datapool_ESRD_new = datapool_ESRD.merge(datapool_ESRD_patients_eGFR['patient_sk'], on = 'patient_sk', how = 'inner')\n",
    "\n",
    "datapool_ESRD = datapool_ESRD_new\n",
    "datapool_ESRD = datapool_ESRD.drop_duplicates()\n",
    "patients_list_ESRD = list(set(np.unique(list(datapool_ESRD['patient_sk']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datapool_control.patient_sk.unique().shape[0])\n",
    "print(datapool_ESRD.patient_sk.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mu and sigma\n",
    "\n",
    "var_list = []\n",
    "n_list = []\n",
    "\n",
    "mu = np.mean(datapool_control['eGFR_EPI'])\n",
    "\n",
    "var_list = datapool_control.groupby('patient_sk').agg({'eGFR_EPI':'std'})\n",
    "var_list = list(var_list.eGFR_EPI)\n",
    "\n",
    "n_list =  datapool_control.groupby('patient_sk').agg({'patient_sk':'count'})\n",
    "n_list = list(n_list.patient_sk)\n",
    "#calculating the mean and variance of the Normal sample\n",
    "\n",
    "n_1 = list((n_list - np.ones(len(n_list))).astype('int'))\n",
    "numerator = np.multiply(n_1, np.power(var_list, 2))\n",
    "denominator = sum(n_list) - len(n_list)\n",
    "sigma = np.power(sum(numerator)/denominator,0.5)\n",
    "\n",
    "print(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametrs:\n",
    "\n",
    "V0 = 0\n",
    "w = 0.75\n",
    "T = -4\n",
    "a = 0.2\n",
    "\n",
    "\n",
    "## Zi:\n",
    "\n",
    "datapool_control['Zi'] = list((datapool_control.eGFR_EPI - mu)/sigma)\n",
    "datapool_ESRD['Zi'] = list((datapool_ESRD.eGFR_EPI - mu)/sigma)\n",
    "\n",
    "## AND let us start palying with Zi and Vi :) AND THE SLOPES AS WELL\n",
    "from numba import jit\n",
    "@jit(nopython=True)\n",
    "\n",
    "def Vi_creator(Zi, patient_sk):\n",
    "    Vi = np.zeros(Zi.shape)\n",
    "    Vi[0] = V0\n",
    "\n",
    "    for i in range(1, Vi.shape[0]):\n",
    "        if patient_sk[i] == patient_sk[i-1]:\n",
    "            Vi[i] = (min(0.0, Zi[i] + w + Vi[i-1]))\n",
    "        else:\n",
    "            Vi[i] = V0\n",
    "            \n",
    "    return Vi\n",
    "\n",
    "datapool_control['Vi'] = Vi_creator(datapool_control['Zi'].values, datapool_control['patient_sk'].values)\n",
    "datapool_ESRD['Vi'] = Vi_creator(datapool_ESRD['Zi'].values, datapool_ESRD['patient_sk'].values)\n",
    "\n",
    "### the algorithm\n",
    "\n",
    "Inst_slope_initial = 0.0\n",
    "Smooth_slope_initial = 0.0\n",
    "\n",
    "@jit(nopython=True)\n",
    "def Slope_creator(Vi, patient_sk, Date_seconds, eGFR_EPI):\n",
    "    Inst_slope = np.zeros(Vi.shape)\n",
    "    Smooth_slope = np.zeros(Vi.shape)\n",
    "    \n",
    "    Inst_slope[0] = Inst_slope_initial\n",
    "    Smooth_slope[0] = Smooth_slope_initial\n",
    "\n",
    "    for i in range(1, Vi.shape[0]):\n",
    "        if patient_sk[i] == patient_sk[i-1]:\n",
    "            if Vi[i-1] == 0.0 :\n",
    "                if Date_seconds[i] - Date_seconds[i-1] != 0:\n",
    "                    Inst_slope[i] = min(0.0, (eGFR_EPI[i] - mu)/((Date_seconds[i] - Date_seconds[i-1])/86400))\n",
    "                else:\n",
    "                    Inst_slope[i] = 0.0\n",
    "            else:\n",
    "                if Date_seconds[i] - Date_seconds[i-1] != 0:\n",
    "                    Inst_slope[i] = min(0.0, (eGFR_EPI[i] - eGFR_EPI[i-1])/((Date_seconds[i] - Date_seconds[i-1])/86400))\n",
    "                else:\n",
    "                    Inst_slope[i] = 0.0\n",
    "            if Date_seconds[i] - Date_seconds[i-1] != 0:\n",
    "                Smooth_slope[i] = (1-a) * Smooth_slope[i-1] + a * (min(0.0, (eGFR_EPI[i] - eGFR_EPI[i-1])/((Date_seconds[i] - Date_seconds[i-1])/86400)))\n",
    "            else:\n",
    "                Smooth_slope[i] = Smooth_slope[i-1]\n",
    "        else:\n",
    "            Inst_slope[i] = Inst_slope_initial\n",
    "            Smooth_slope[i] = Smooth_slope_initial\n",
    "            \n",
    "    return [Inst_slope, Smooth_slope]\n",
    "\n",
    "[datapool_control['Inst_slope'],datapool_control['Smooth_slope']] = Slope_creator(datapool_control['Vi'].values, datapool_control['patient_sk'].values, datapool_control['Date_seconds'].values, datapool_control['eGFR_EPI'].values)\n",
    "[datapool_ESRD['Inst_slope'], datapool_ESRD['Smooth_slope']] = Slope_creator(datapool_ESRD['Vi'].values, datapool_ESRD['patient_sk'].values, datapool_ESRD['Date_seconds'].values, datapool_ESRD['eGFR_EPI'].values)\n",
    "\n",
    "# Making up the result trigger date and eGFR tables\n",
    "\n",
    "patients_control_trigger = datapool_control[datapool_control['Vi'] <= T].groupby('patient_sk').agg({'Date': 'min'})\n",
    "patients_control_trigger = patients_control_trigger.reset_index()\n",
    "patients_control_trigger = patients_control_trigger.merge(datapool_control[['patient_sk', 'eGFR_EPI', 'Date', 'Inst_slope', 'Smooth_slope']], on=['patient_sk'], how='inner')\n",
    "patients_control_trigger = patients_control_trigger.rename({'Date_x':'Trigger_date'}, axis = 1)\n",
    "patients_control_trigger = patients_control_trigger.rename({'Date_y':'Date'}, axis = 1)\n",
    "patients_control_trigger = patients_control_trigger[patients_control_trigger.Trigger_date == patients_control_trigger.Date]\n",
    "patients_control_trigger['New_label'] = list(np.ones(patients_control_trigger.patient_sk.shape[0]))\n",
    "\n",
    "patients_ESRD_trigger = datapool_ESRD[datapool_ESRD['Vi'] <= T].groupby('patient_sk').agg({'Date': 'min'})\n",
    "patients_ESRD_trigger = patients_ESRD_trigger.reset_index()\n",
    "patients_ESRD_trigger = patients_ESRD_trigger.merge(datapool_ESRD[['patient_sk', 'eGFR_EPI', 'Date', 'Inst_slope', 'Smooth_slope']], on=['patient_sk'], how='inner')\n",
    "patients_ESRD_trigger = patients_ESRD_trigger.rename({'Date_x':'Trigger_date'}, axis = 1)\n",
    "patients_ESRD_trigger = patients_ESRD_trigger.rename({'Date_y':'Date'}, axis = 1)\n",
    "patients_ESRD_trigger = patients_ESRD_trigger[patients_ESRD_trigger.Trigger_date == patients_ESRD_trigger.Date]\n",
    "patients_ESRD_trigger['New_label'] = list(np.ones(patients_ESRD_trigger.patient_sk.shape[0]))\n",
    "\n",
    "\n",
    "#Labeling and finishing :)\n",
    "\n",
    "patients_Normal_labeled = pd.DataFrame({'patient_sk' : list(datapool_control.patient_sk.unique()) , 'Label' : list(np.ones(len(list(datapool_control.patient_sk.unique()))))}) \n",
    "\n",
    "patients_Normal_labeled =  patients_Normal_labeled.merge(patients_control_trigger, on='patient_sk', how='left')\n",
    "patients_Normal_labeled = patients_Normal_labeled.drop_duplicates('patient_sk')\n",
    "patients_Normal_labeled = patients_Normal_labeled.drop('Date', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "patients_ESRD_labeled = pd.DataFrame({'patient_sk' : list(datapool_ESRD.patient_sk.unique()) , 'Label' : list(np.ones(len(list(datapool_ESRD.patient_sk.unique()))))}) \n",
    "\n",
    "patients_ESRD_labeled =  patients_ESRD_labeled.merge(patients_ESRD_trigger, on='patient_sk', how='left')\n",
    "patients_ESRD_labeled = patients_ESRD_labeled.drop_duplicates('patient_sk')\n",
    "patients_ESRD_labeled = patients_ESRD_labeled.drop('Date', axis = 1)\n",
    "\n",
    "#Accuracy = true(positive and negative)/total population\n",
    "# ESRD NaN = 0.0\n",
    "# Normal NaN = 0.0\n",
    "\n",
    "#RIGHT detection in ESRD:\n",
    "numbet_of_ones_ESRD = patients_ESRD_labeled[patients_ESRD_labeled['New_label'] == 1].shape[0]\n",
    "\n",
    "#WRONG detection in Normal\n",
    "numbet_of_ones_Normal = patients_Normal_labeled[patients_Normal_labeled['New_label'] == 1].shape[0]\n",
    "\n",
    "total_ESRD = patients_ESRD_labeled.shape[0]\n",
    "total_Normal = patients_Normal_labeled.shape[0]\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "Accuracy = (numbet_of_ones_ESRD + (total_Normal - numbet_of_ones_Normal))/(total_ESRD + total_Normal)\n",
    "\n",
    "#Sensetivity\n",
    "tp = numbet_of_ones_ESRD\n",
    "fn = total_ESRD - numbet_of_ones_ESRD\n",
    "Sensetivity = tp/(tp+fn)\n",
    "\n",
    "#Specificity\n",
    "tn = total_Normal - numbet_of_ones_Normal\n",
    "fp = numbet_of_ones_Normal\n",
    "Specificity = tn/(tn+fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensetivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, creating the MERGED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_ESRD_full_dates_pandas = pd.read_csv('Final_patients_ESRD_full_dates_pandas.csv')\n",
    "patients_ESRD_full_dates_pandas = patients_ESRD_full_dates_pandas.drop(patients_ESRD_full_dates_pandas.columns[[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = patients_ESRD_labeled.merge(patients_ESRD_full_dates_pandas, on = 'patient_sk' , how = 'inner')\n",
    "merged_dataset['Trigger_date'] = pd.to_datetime(merged_dataset['Trigger_date'], errors='coerce')\n",
    "merged_dataset['Diagnosis_admission_date_ESRD'] = pd.to_datetime(merged_dataset['Diagnosis_admission_date_ESRD'], errors='coerce')\n",
    "merged_dataset['Diagnosis_admission_date_dialysis'] = pd.to_datetime(merged_dataset['Diagnosis_admission_date_dialysis'], errors='coerce')\n",
    "merged_dataset['Diagnosis_admission_date_transplant'] = pd.to_datetime(merged_dataset['Diagnosis_admission_date_transplant'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lislis_ESRD = (merged_dataset['Diagnosis_admission_date_ESRD'] - merged_dataset['Trigger_date'])\n",
    "lislis_dialysis = (merged_dataset['Diagnosis_admission_date_dialysis'] - merged_dataset['Trigger_date'])\n",
    "lislis_transplant = (merged_dataset['Diagnosis_admission_date_transplant'] - merged_dataset['Trigger_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset['time_to_event_ESRD'] = lislis_ESRD\n",
    "merged_dataset['time_to_event_dialysis'] = lislis_dialysis\n",
    "merged_dataset['time_to_event_transplant'] = lislis_transplant\n",
    "\n",
    "#Making the negatives, positive\n",
    "for i in range(len(lislis_ESRD)):\n",
    "    if lislis_ESRD[i] <= datetime.timedelta(0):\n",
    "        merged_dataset['time_to_event_ESRD'][i] = datetime.timedelta(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.to_csv('merged_dataset_dates_timedeltas_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For further information please contact rzz5164@psu.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
